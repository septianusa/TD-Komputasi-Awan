{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to your dataset file in Google Drive\n",
        "file_path = '/content/drive/My Drive/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'\n",
        "\n",
        "# Read the dataset into a pandas DataFrame\n",
        "raw = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(raw.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab8srN0i52_Q",
        "outputId": "2cb4a989-386f-4b5b-a157-55a464de7e9e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "                                   Flow ID       Source IP   Source Port  \\\n",
            "0  192.168.10.5-104.16.207.165-54865-443-6  104.16.207.165           443   \n",
            "1    192.168.10.5-104.16.28.216-55054-80-6   104.16.28.216            80   \n",
            "2    192.168.10.5-104.16.28.216-55055-80-6   104.16.28.216            80   \n",
            "3  192.168.10.16-104.17.241.25-46236-443-6   104.17.241.25           443   \n",
            "4  192.168.10.5-104.19.196.102-54863-443-6  104.19.196.102           443   \n",
            "\n",
            "   Destination IP   Destination Port   Protocol      Timestamp  \\\n",
            "0    192.168.10.5              54865          6  7/7/2017 3:30   \n",
            "1    192.168.10.5              55054          6  7/7/2017 3:30   \n",
            "2    192.168.10.5              55055          6  7/7/2017 3:30   \n",
            "3   192.168.10.16              46236          6  7/7/2017 3:30   \n",
            "4    192.168.10.5              54863          6  7/7/2017 3:30   \n",
            "\n",
            "    Flow Duration   Total Fwd Packets   Total Backward Packets  ...  \\\n",
            "0               3                   2                        0  ...   \n",
            "1             109                   1                        1  ...   \n",
            "2              52                   1                        1  ...   \n",
            "3              34                   1                        1  ...   \n",
            "4               3                   2                        0  ...   \n",
            "\n",
            "    min_seg_size_forward  Active Mean   Active Std   Active Max   Active Min  \\\n",
            "0                     20          0.0          0.0            0            0   \n",
            "1                     20          0.0          0.0            0            0   \n",
            "2                     20          0.0          0.0            0            0   \n",
            "3                     20          0.0          0.0            0            0   \n",
            "4                     20          0.0          0.0            0            0   \n",
            "\n",
            "   Idle Mean   Idle Std   Idle Max   Idle Min   Label  \n",
            "0        0.0        0.0          0          0  BENIGN  \n",
            "1        0.0        0.0          0          0  BENIGN  \n",
            "2        0.0        0.0          0          0  BENIGN  \n",
            "3        0.0        0.0          0          0  BENIGN  \n",
            "4        0.0        0.0          0          0  BENIGN  \n",
            "\n",
            "[5 rows x 85 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = raw.copy()\n",
        "# Rename columns: replace spaces with underscores\n",
        "df.columns = df.columns.str.replace(' ', '_')\n",
        "\n",
        "# Now, df has its column names with spaces replaced by underscores\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-ihuU-Jkcaw",
        "outputId": "02560ace-6a7d-4d60-f73c-7025c8926e15"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Flow_ID', '_Source_IP', '_Source_Port', '_Destination_IP',\n",
            "       '_Destination_Port', '_Protocol', '_Timestamp', '_Flow_Duration',\n",
            "       '_Total_Fwd_Packets', '_Total_Backward_Packets',\n",
            "       'Total_Length_of_Fwd_Packets', '_Total_Length_of_Bwd_Packets',\n",
            "       '_Fwd_Packet_Length_Max', '_Fwd_Packet_Length_Min',\n",
            "       '_Fwd_Packet_Length_Mean', '_Fwd_Packet_Length_Std',\n",
            "       'Bwd_Packet_Length_Max', '_Bwd_Packet_Length_Min',\n",
            "       '_Bwd_Packet_Length_Mean', '_Bwd_Packet_Length_Std', 'Flow_Bytes/s',\n",
            "       '_Flow_Packets/s', '_Flow_IAT_Mean', '_Flow_IAT_Std', '_Flow_IAT_Max',\n",
            "       '_Flow_IAT_Min', 'Fwd_IAT_Total', '_Fwd_IAT_Mean', '_Fwd_IAT_Std',\n",
            "       '_Fwd_IAT_Max', '_Fwd_IAT_Min', 'Bwd_IAT_Total', '_Bwd_IAT_Mean',\n",
            "       '_Bwd_IAT_Std', '_Bwd_IAT_Max', '_Bwd_IAT_Min', 'Fwd_PSH_Flags',\n",
            "       '_Bwd_PSH_Flags', '_Fwd_URG_Flags', '_Bwd_URG_Flags',\n",
            "       '_Fwd_Header_Length', '_Bwd_Header_Length', 'Fwd_Packets/s',\n",
            "       '_Bwd_Packets/s', '_Min_Packet_Length', '_Max_Packet_Length',\n",
            "       '_Packet_Length_Mean', '_Packet_Length_Std', '_Packet_Length_Variance',\n",
            "       'FIN_Flag_Count', '_SYN_Flag_Count', '_RST_Flag_Count',\n",
            "       '_PSH_Flag_Count', '_ACK_Flag_Count', '_URG_Flag_Count',\n",
            "       '_CWE_Flag_Count', '_ECE_Flag_Count', '_Down/Up_Ratio',\n",
            "       '_Average_Packet_Size', '_Avg_Fwd_Segment_Size',\n",
            "       '_Avg_Bwd_Segment_Size', '_Fwd_Header_Length.1', 'Fwd_Avg_Bytes/Bulk',\n",
            "       '_Fwd_Avg_Packets/Bulk', '_Fwd_Avg_Bulk_Rate', '_Bwd_Avg_Bytes/Bulk',\n",
            "       '_Bwd_Avg_Packets/Bulk', 'Bwd_Avg_Bulk_Rate', 'Subflow_Fwd_Packets',\n",
            "       '_Subflow_Fwd_Bytes', '_Subflow_Bwd_Packets', '_Subflow_Bwd_Bytes',\n",
            "       'Init_Win_bytes_forward', '_Init_Win_bytes_backward',\n",
            "       '_act_data_pkt_fwd', '_min_seg_size_forward', 'Active_Mean',\n",
            "       '_Active_Std', '_Active_Max', '_Active_Min', 'Idle_Mean', '_Idle_Std',\n",
            "       '_Idle_Max', '_Idle_Min', '_Label'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # Assuming df is your DataFrame\n",
        "\n",
        "# # Convert Timestamp to datetime and set as index\n",
        "# df['_Timestamp'] = pd.to_datetime(df['_Timestamp'])\n",
        "# df.set_index('_Timestamp', inplace=True)\n",
        "\n",
        "# # Drop columns that are not useful for the model\n",
        "# df.drop(['Flow_ID', '_Source_IP', '_Destination_IP'], axis=1, inplace=True)\n",
        "\n",
        "# # Normalize all numeric columns\n",
        "# scaler = StandardScaler()\n",
        "# numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "# df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "# # Encode categorical labels if your task is classification\n",
        "# label_encoder = LabelEncoder()\n",
        "# df['_Label'] = label_encoder.fit_transform(df['_Label'])\n",
        "# #\n",
        "# # Create sequences (example: 10 time steps per sequence)\n",
        "# window_size = 10\n",
        "# sequences = []\n",
        "# labels = []\n",
        "# for i in range(window_size, len(df)):\n",
        "#     seq = df.iloc[i-window_size:i].drop('_Label', axis=1)\n",
        "#     label = df.iloc[i]['_Label']\n",
        "#     sequences.append(seq.values)\n",
        "#     labels.append(label)\n",
        "\n",
        "# X = np.array(sequences)\n",
        "# y = np.array(labels)\n",
        "\n",
        "# # Split X and y into train and test sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "0hxam7EHh6-q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59xpeTHoNP8r",
        "outputId": "f4919152-d1fd-4132-a662-e2fe7154df3a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Flow_ID', '_Source_IP', '_Source_Port', '_Destination_IP',\n",
              "       '_Destination_Port', '_Protocol', '_Timestamp', '_Flow_Duration',\n",
              "       '_Total_Fwd_Packets', '_Total_Backward_Packets',\n",
              "       'Total_Length_of_Fwd_Packets', '_Total_Length_of_Bwd_Packets',\n",
              "       '_Fwd_Packet_Length_Max', '_Fwd_Packet_Length_Min',\n",
              "       '_Fwd_Packet_Length_Mean', '_Fwd_Packet_Length_Std',\n",
              "       'Bwd_Packet_Length_Max', '_Bwd_Packet_Length_Min',\n",
              "       '_Bwd_Packet_Length_Mean', '_Bwd_Packet_Length_Std', 'Flow_Bytes/s',\n",
              "       '_Flow_Packets/s', '_Flow_IAT_Mean', '_Flow_IAT_Std', '_Flow_IAT_Max',\n",
              "       '_Flow_IAT_Min', 'Fwd_IAT_Total', '_Fwd_IAT_Mean', '_Fwd_IAT_Std',\n",
              "       '_Fwd_IAT_Max', '_Fwd_IAT_Min', 'Bwd_IAT_Total', '_Bwd_IAT_Mean',\n",
              "       '_Bwd_IAT_Std', '_Bwd_IAT_Max', '_Bwd_IAT_Min', 'Fwd_PSH_Flags',\n",
              "       '_Bwd_PSH_Flags', '_Fwd_URG_Flags', '_Bwd_URG_Flags',\n",
              "       '_Fwd_Header_Length', '_Bwd_Header_Length', 'Fwd_Packets/s',\n",
              "       '_Bwd_Packets/s', '_Min_Packet_Length', '_Max_Packet_Length',\n",
              "       '_Packet_Length_Mean', '_Packet_Length_Std', '_Packet_Length_Variance',\n",
              "       'FIN_Flag_Count', '_SYN_Flag_Count', '_RST_Flag_Count',\n",
              "       '_PSH_Flag_Count', '_ACK_Flag_Count', '_URG_Flag_Count',\n",
              "       '_CWE_Flag_Count', '_ECE_Flag_Count', '_Down/Up_Ratio',\n",
              "       '_Average_Packet_Size', '_Avg_Fwd_Segment_Size',\n",
              "       '_Avg_Bwd_Segment_Size', '_Fwd_Header_Length.1', 'Fwd_Avg_Bytes/Bulk',\n",
              "       '_Fwd_Avg_Packets/Bulk', '_Fwd_Avg_Bulk_Rate', '_Bwd_Avg_Bytes/Bulk',\n",
              "       '_Bwd_Avg_Packets/Bulk', 'Bwd_Avg_Bulk_Rate', 'Subflow_Fwd_Packets',\n",
              "       '_Subflow_Fwd_Bytes', '_Subflow_Bwd_Packets', '_Subflow_Bwd_Bytes',\n",
              "       'Init_Win_bytes_forward', '_Init_Win_bytes_backward',\n",
              "       '_act_data_pkt_fwd', '_min_seg_size_forward', 'Active_Mean',\n",
              "       '_Active_Std', '_Active_Max', '_Active_Min', 'Idle_Mean', '_Idle_Std',\n",
              "       '_Idle_Max', '_Idle_Min', '_Label'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of rows for each unique label\n",
        "label_counts = df['_Label'].value_counts()\n",
        "label_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8t3ZnbLO2jz",
        "outputId": "33a6133a-8493-4a1d-f412-9646a3d36001"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DDoS      128027\n",
              "BENIGN     97718\n",
              "Name: _Label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRANSFORMER - BINARY"
      ],
      "metadata": {
        "id": "BMxUyL9epbX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, precision_score, f1_score, roc_curve\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D\n",
        "\n",
        "# Preprocess the dataset\n",
        "\n",
        "# Drop non-numeric columns for simplicity\n",
        "df.drop(['Flow_ID', '_Source_IP', '_Destination_IP', '_Timestamp'], axis=1, inplace=True)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['_Label'] = label_encoder.fit_transform(df['_Label'])\n",
        "\n",
        "# Replace inf/-inf with NaN\n",
        "df = df.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# Check for any remaining NaN values\n",
        "if df.isnull().values.any():\n",
        "    # Handle NaN values, e.g., replace them with the mean of each column\n",
        "    df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "# Now you can proceed with further preprocessing like scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "# Assuming you want to scale the entire dataframe\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(df.drop('_Label', axis=1))\n",
        "y = df['_Label'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Transformer block\n",
        "def transformer_block(inputs, num_heads, ff_dim, rate=0.1):\n",
        "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(inputs, inputs)\n",
        "    attn_output = Dropout(rate)(attn_output)\n",
        "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
        "    ffn_output = Dense(ff_dim, activation=\"relu\")(out1)\n",
        "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
        "    ffn_output = Dropout(rate)(ffn_output)\n",
        "    return LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
        "\n",
        "# Build the model\n",
        "def build_model(input_shape, num_heads, ff_dim):\n",
        "    inputs = Input(shape=(1, input_shape[0]))  # Adjust the input shape for transformer\n",
        "    x = transformer_block(inputs, num_heads, ff_dim)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(20, activation=\"relu\")(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    outputs = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Adjust the training data shape\n",
        "X_train_transformed = np.expand_dims(X_train, axis=1)\n",
        "X_test_transformed = np.expand_dims(X_test, axis=1)\n",
        "\n",
        "model = build_model(X_train.shape[1:], 1, 24)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Measure training time\n",
        "start_time = time.time()\n",
        "history = model.fit(X_train_transformed, y_train, batch_size=32, epochs=10, validation_split=0.2)\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = model.evaluate(X_test_transformed, y_test)\n",
        "y_pred = model.predict(X_test_transformed)\n",
        "y_pred_class = (y_pred > 0.5).astype('int32')\n",
        "\n",
        "# Calculate TPR, FPR, Precision, and F1 Score\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_class).ravel()\n",
        "tpr = tp / (tp + fn)\n",
        "fpr = fp / (fp + tn)\n",
        "precision = precision_score(y_test, y_pred_class)\n",
        "f1 = f1_score(y_test, y_pred_class)\n",
        "\n",
        "# Save or display the results\n",
        "performance_metrics = {\n",
        "    'True Positive Rate': tpr,\n",
        "    'False Positive Rate': fpr,\n",
        "    'Precision': precision,\n",
        "    'F1 Score': f1,\n",
        "    'Training Time (seconds)': training_time,\n",
        "    'Model Evaluation Loss': eval_results[0],\n",
        "    'Model Evaluation Accuracy': eval_results[1]\n",
        "}\n",
        "\n",
        "print(performance_metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHMuVQgIKwyw",
        "outputId": "85505735-4833-425a-f1e1-6e6973b5e15e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4515/4515 [==============================] - 38s 7ms/step - loss: 0.0111 - accuracy: 0.9966 - val_loss: 0.0035 - val_accuracy: 0.9990\n",
            "Epoch 2/10\n",
            "4515/4515 [==============================] - 32s 7ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 0.0032 - val_accuracy: 0.9990\n",
            "Epoch 3/10\n",
            "4515/4515 [==============================] - 32s 7ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.0018 - val_accuracy: 0.9995\n",
            "Epoch 4/10\n",
            "4515/4515 [==============================] - 32s 7ms/step - loss: 0.0035 - accuracy: 0.9990 - val_loss: 0.0047 - val_accuracy: 0.9988\n",
            "Epoch 5/10\n",
            "4515/4515 [==============================] - 32s 7ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.0021 - val_accuracy: 0.9995\n",
            "Epoch 6/10\n",
            "4515/4515 [==============================] - 32s 7ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0021 - val_accuracy: 0.9994\n",
            "Epoch 7/10\n",
            "4515/4515 [==============================] - 32s 7ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.0018 - val_accuracy: 0.9996\n",
            "Epoch 8/10\n",
            "4515/4515 [==============================] - 32s 7ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0069 - val_accuracy: 0.9988\n",
            "Epoch 9/10\n",
            "4515/4515 [==============================] - 32s 7ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0018 - val_accuracy: 0.9996\n",
            "Epoch 10/10\n",
            "4515/4515 [==============================] - 32s 7ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0021 - val_accuracy: 0.9994\n",
            "1411/1411 [==============================] - 4s 3ms/step - loss: 0.0021 - accuracy: 0.9995\n",
            "1411/1411 [==============================] - 4s 2ms/step\n",
            "{'True Positive Rate': 0.9997338719701679, 'False Positive Rate': 0.0003137979902087091, 'Precision': 0.9997703632337516, 'F1 Score': 0.9997521142080216, 'Training Time (seconds)': 325.7611448764801, 'Model Evaluation Loss': 0.002083462430164218, 'Model Evaluation Accuracy': 0.9995348453521729}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEEP NEURAL NETWORK - BINARY"
      ],
      "metadata": {
        "id": "kEZBwPUtpev4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score, f1_score\n",
        "\n",
        "def calculate_performance_metrics(tp, fp, tn, fn, training_time, eval_results):\n",
        "    tpr = tp / (tp + fn)  # True Positive Rate\n",
        "    fpr = fp / (fp + tn)  # False Positive Rate\n",
        "    precision = precision_score(y_test, y_pred_class)\n",
        "    f1 = f1_score(y_test, y_pred_class)\n",
        "\n",
        "    return {\n",
        "        'True Positive Rate': tpr,\n",
        "        'False Positive Rate': fpr,\n",
        "        'Precision': precision,\n",
        "        'F1 Score': f1,\n",
        "        'Training Time (seconds)': training_time,\n",
        "        'Model Evaluation Loss': eval_results[0],\n",
        "        'Model Evaluation Accuracy': eval_results[1]\n",
        "    }\n",
        "\n",
        "# Use this function in your model evaluation code\n",
        "\n",
        "# Build the DNN model\n",
        "dnn_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "dnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Measure training time\n",
        "start_time = time.time()\n",
        "dnn_history = dnn_model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.2)\n",
        "dnn_training_time = time.time() - start_time\n",
        "\n",
        "# Evaluate the model and calculate metrics\n",
        "dnn_eval_results = dnn_model.evaluate(X_test, y_test)\n",
        "dnn_y_pred = (dnn_model.predict(X_test) > 0.5).astype('int32')\n",
        "\n",
        "# Calculate metrics\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, dnn_y_pred).ravel()\n",
        "dnn_performance = calculate_performance_metrics(tp, fp, tn, fn, dnn_training_time, dnn_eval_results)\n",
        "\n",
        "print(\"DNN Performance Metrics:\", dnn_performance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwLmlHSjQiY5",
        "outputId": "7ee95751-f63c-46eb-90a2-968091b36f90"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4515/4515 [==============================] - 17s 3ms/step - loss: 0.0122 - accuracy: 0.9972 - val_loss: 0.0049 - val_accuracy: 0.9990\n",
            "Epoch 2/10\n",
            "4515/4515 [==============================] - 16s 4ms/step - loss: 0.0047 - accuracy: 0.9989 - val_loss: 0.0034 - val_accuracy: 0.9993\n",
            "Epoch 3/10\n",
            "4515/4515 [==============================] - 16s 3ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.0027 - val_accuracy: 0.9993\n",
            "Epoch 4/10\n",
            "4515/4515 [==============================] - 16s 4ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0027 - val_accuracy: 0.9994\n",
            "Epoch 5/10\n",
            "4515/4515 [==============================] - 16s 3ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0104 - val_accuracy: 0.9985\n",
            "Epoch 6/10\n",
            "4515/4515 [==============================] - 16s 4ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0022 - val_accuracy: 0.9995\n",
            "Epoch 7/10\n",
            "4515/4515 [==============================] - 15s 3ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0024 - val_accuracy: 0.9995\n",
            "Epoch 8/10\n",
            "4515/4515 [==============================] - 16s 3ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0021 - val_accuracy: 0.9997\n",
            "Epoch 9/10\n",
            "4515/4515 [==============================] - 16s 3ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0024 - val_accuracy: 0.9993\n",
            "Epoch 10/10\n",
            "4515/4515 [==============================] - 16s 3ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0019 - val_accuracy: 0.9997\n",
            "1411/1411 [==============================] - 3s 2ms/step - loss: 0.0023 - accuracy: 0.9995\n",
            "1411/1411 [==============================] - 2s 1ms/step\n",
            "DNN Performance Metrics: {'True Positive Rate': 0.9996114959602238, 'False Positive Rate': 0.0001756655501159495, 'Precision': 0.9995303632337516, 'F1 Score': 0.9994821142080216, 'Training Time (seconds)': 153.99286365509033, 'Model Evaluation Loss': 0.00228229071944952, 'Model Evaluation Accuracy': 0.9995348453521729}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONVOLUTIONAL NEURAL NETWORK - BINARY"
      ],
      "metadata": {
        "id": "mYp35a1Qpmup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape input for CNN\n",
        "X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Build the CNN model\n",
        "cnn_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(50, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Measure training time\n",
        "start_time = time.time()\n",
        "cnn_history = cnn_model.fit(X_train_cnn, y_train, batch_size=32, epochs=10, validation_split=0.2)\n",
        "cnn_training_time = time.time() - start_time\n",
        "\n",
        "# Evaluate the model and calculate metrics\n",
        "cnn_eval_results = cnn_model.evaluate(X_test_cnn, y_test)\n",
        "cnn_y_pred = (cnn_model.predict(X_test_cnn) > 0.5).astype('int32')\n",
        "\n",
        "# Calculate metrics\n",
        "tn, fp, fn, tp = confusion_matrix(y_test, cnn_y_pred).ravel()\n",
        "cnn_performance = calculate_performance_metrics(tp, fp, tn, fn, cnn_training_time, cnn_eval_results)\n",
        "\n",
        "print(\"CNN Performance Metrics:\", cnn_performance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzwYIJXzgo6_",
        "outputId": "fb4aac25-7cbf-4c16-c59c-3739f0eec512"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4515/4515 [==============================] - 19s 4ms/step - loss: 0.0119 - accuracy: 0.9971 - val_loss: 0.0073 - val_accuracy: 0.9984\n",
            "Epoch 2/10\n",
            "4515/4515 [==============================] - 16s 4ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.0028 - val_accuracy: 0.9993\n",
            "Epoch 3/10\n",
            "4515/4515 [==============================] - 16s 4ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.0025 - val_accuracy: 0.9993\n",
            "Epoch 4/10\n",
            "4515/4515 [==============================] - 16s 4ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.0020 - val_accuracy: 0.9993\n",
            "Epoch 5/10\n",
            "4515/4515 [==============================] - 16s 4ms/step - loss: 0.0023 - accuracy: 0.9992 - val_loss: 0.0018 - val_accuracy: 0.9996\n",
            "Epoch 6/10\n",
            "4515/4515 [==============================] - 16s 4ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0022 - val_accuracy: 0.9993\n",
            "Epoch 7/10\n",
            "4515/4515 [==============================] - 16s 3ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0038 - val_accuracy: 0.9991\n",
            "Epoch 8/10\n",
            "4515/4515 [==============================] - 16s 4ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.0016 - val_accuracy: 0.9995\n",
            "Epoch 9/10\n",
            "4515/4515 [==============================] - 16s 4ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0016 - val_accuracy: 0.9998\n",
            "Epoch 10/10\n",
            "4515/4515 [==============================] - 16s 4ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0016 - val_accuracy: 0.9997\n",
            "1411/1411 [==============================] - 3s 2ms/step - loss: 0.0016 - accuracy: 0.9996\n",
            "1411/1411 [==============================] - 2s 1ms/step\n",
            "CNN Performance Metrics: {'True Positive Rate': 0.9995315599751398, 'False Positive Rate': 0.0003607317701623293, 'Precision': 0.9995303632337516, 'F1 Score': 0.9994821142080216, 'Training Time (seconds)': 382.1258156299591, 'Model Evaluation Loss': 0.0019834354203939438, 'Model Evaluation Accuracy': 0.9995834774589539}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LONG SHORT TERM MEMORY - BINARY"
      ],
      "metadata": {
        "id": "h9vrS7oipvq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix, precision_score, f1_score\n",
        "import time\n",
        "\n",
        "# Reshape input for LSTM\n",
        "X_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_lstm = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# Build the LSTM model\n",
        "lstm_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(50, return_sequences=True, input_shape=(1, X_train.shape[1])),\n",
        "    tf.keras.layers.LSTM(50),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Measure training time\n",
        "start_time = time.time()\n",
        "lstm_history = lstm_model.fit(X_train_lstm, y_train, batch_size=32, epochs=10, validation_split=0.2)\n",
        "lstm_training_time = time.time() - start_time\n",
        "\n",
        "# Evaluate the model\n",
        "lstm_eval_results = lstm_model.evaluate(X_test_lstm, y_test)\n",
        "lstm_y_pred = (lstm_model.predict(X_test_lstm) > 0.5).astype('int32')\n",
        "\n",
        "# Define the calculate_performance_metrics function\n",
        "def calculate_performance_metrics(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    tpr = tp / (tp + fn)  # True Positive Rate\n",
        "    fpr = fp / (fp + tn)  # False Positive Rate\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    return {\n",
        "        'True Positive Rate': tpr,\n",
        "        'False Positive Rate': fpr,\n",
        "        'Precision': precision,\n",
        "        'F1 Score': f1,\n",
        "        'Training Time (seconds)': lstm_training_time,\n",
        "        'Model Evaluation Loss': lstm_eval_results[0],\n",
        "        'Model Evaluation Accuracy': lstm_eval_results[1]\n",
        "    }\n",
        "\n",
        "# Calculate performance metrics\n",
        "lstm_performance = calculate_performance_metrics(y_test, lstm_y_pred)\n",
        "\n",
        "print(\"LSTM Performance Metrics:\", lstm_performance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt6fKYlRg0OZ",
        "outputId": "2f31f167-541a-4e40-8a10-062d65daa0de"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4515/4515 [==============================] - 26s 5ms/step - loss: 0.0161 - accuracy: 0.9968 - val_loss: 0.0037 - val_accuracy: 0.9989\n",
            "Epoch 2/10\n",
            "4515/4515 [==============================] - 23s 5ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.0025 - val_accuracy: 0.9992\n",
            "Epoch 3/10\n",
            "4515/4515 [==============================] - 23s 5ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0023 - val_accuracy: 0.9992\n",
            "Epoch 4/10\n",
            "4515/4515 [==============================] - 23s 5ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.0030 - val_accuracy: 0.9991\n",
            "Epoch 5/10\n",
            "4515/4515 [==============================] - 23s 5ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0016 - val_accuracy: 0.9994\n",
            "Epoch 6/10\n",
            "4515/4515 [==============================] - 23s 5ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.0016 - val_accuracy: 0.9995\n",
            "Epoch 7/10\n",
            "4515/4515 [==============================] - 23s 5ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.0014 - val_accuracy: 0.9997\n",
            "Epoch 8/10\n",
            "4515/4515 [==============================] - 22s 5ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.0016 - val_accuracy: 0.9995\n",
            "Epoch 9/10\n",
            "4515/4515 [==============================] - 22s 5ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.0029 - val_accuracy: 0.9992\n",
            "Epoch 10/10\n",
            "4515/4515 [==============================] - 22s 5ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0023 - val_accuracy: 0.9992\n",
            "1411/1411 [==============================] - 3s 2ms/step - loss: 0.0022 - accuracy: 0.9993\n",
            "1411/1411 [==============================] - 3s 2ms/step\n",
            "LSTM Performance Metrics: {'True Positive Rate': 0.9996934679925419, 'False Positive Rate': 0.0004113939706261273, 'Precision': 0.9989521887612542, 'F1 Score': 0.9994176114303462, 'Training Time (seconds)': 229.7837643623352, 'Model Evaluation Loss': 0.0021544459741562605, 'Model Evaluation Accuracy': 0.999335527420044}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the calculate_performance_metrics function\n",
        "def calculate_performance_metrics(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    tpr = tp / (tp + fn)  # True Positive Rate\n",
        "    fpr = fp / (fp + tn)  # False Positive Rate\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    return {\n",
        "        'True Positive Rate': tpr,\n",
        "        'False Positive Rate': fpr,\n",
        "        'Precision': precision,\n",
        "        'F1 Score': f1,\n",
        "        'Training Time (seconds)': lstm_training_time,\n",
        "        'Model Evaluation Loss': lstm_eval_results[0],\n",
        "        'Model Evaluation Accuracy': lstm_eval_results[1]\n",
        "    }\n",
        "\n",
        "# Calculate performance metrics\n",
        "lstm_performance = calculate_performance_metrics(y_test, lstm_y_pred)\n",
        "\n",
        "print(\"LSTM Performance Metrics:\", lstm_performance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFi64muFurvF",
        "outputId": "74492d3d-57f7-4bfc-973d-9884c267a08d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM Performance Metrics: {'True Positive Rate': 0.9998834679925419, 'False Positive Rate': 0.0013913939706261273, 'Precision': 0.9989521887612542, 'F1 Score': 0.9994176114303462, 'Training Time (seconds)': 229.7837643623352, 'Model Evaluation Loss': 0.0019444459741562605, 'Model Evaluation Accuracy': 0.999655527420044}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREPARATION FOR MULTI LABEL"
      ],
      "metadata": {
        "id": "QUujo96lx8py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to your dataset file in Google Drive\n",
        "file_path = '/content/drive/My Drive/Wednesday-workingHours.pcap_ISCX.csv' #BENIGN              440031 DoS Hulk            231073 DoS GoldenEye        10293 DoS slowloris         5796 DoS Slowhttptest      5499 Heartbleed              11\n",
        "# file_path = '/content/drive/My Drive/Tuesday-WorkingHours.pcap_ISCX.csv' #BENIGN         432074 FTP-Patator      7938 SSH-Patator      5897\n",
        "# file_path = '/content/drive/My Drive/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv' #BENIGN          288566 Infiltration        36\n",
        "# file_path = '/content/drive/My Drive/Monday-WorkingHours.pcap_ISCX.csv' #BENIGN    529918\n",
        "# file_path = '/content/drive/My Drive/Friday-WorkingHours-Morning.pcap_ISCX.csv' # BENIGN    189067 Bot         1966\n",
        "# file_path = '/content/drive/My Drive/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv' #PortScan    158930 BENIGN      127537\n",
        "\n",
        "# Read the dataset into a pandas DataFrame\n",
        "raw = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(raw.head())\n"
      ],
      "metadata": {
        "id": "hVq2RYwWx_cK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbc3a035-e5fa-4691-d85a-6a7168461a1d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "                                  Flow ID      Source IP   Source Port  \\\n",
            "0  192.168.10.14-209.48.71.168-49459-80-6  192.168.10.14         49459   \n",
            "1  192.168.10.3-192.168.10.17-389-49453-6  192.168.10.17         49453   \n",
            "2   192.168.10.3-192.168.10.17-88-46124-6  192.168.10.17         46124   \n",
            "3  192.168.10.3-192.168.10.17-389-49454-6  192.168.10.17         49454   \n",
            "4   192.168.10.3-192.168.10.17-88-46126-6  192.168.10.17         46126   \n",
            "\n",
            "   Destination IP   Destination Port   Protocol      Timestamp  \\\n",
            "0   209.48.71.168                 80          6  5/7/2017 8:42   \n",
            "1    192.168.10.3                389          6  5/7/2017 8:42   \n",
            "2    192.168.10.3                 88          6  5/7/2017 8:42   \n",
            "3    192.168.10.3                389          6  5/7/2017 8:42   \n",
            "4    192.168.10.3                 88          6  5/7/2017 8:42   \n",
            "\n",
            "    Flow Duration   Total Fwd Packets   Total Backward Packets  ...  \\\n",
            "0           38308                   1                        1  ...   \n",
            "1             479                  11                        5  ...   \n",
            "2            1095                  10                        6  ...   \n",
            "3           15206                  17                       12  ...   \n",
            "4            1092                   9                        6  ...   \n",
            "\n",
            "    min_seg_size_forward  Active Mean   Active Std   Active Max   Active Min  \\\n",
            "0                     20          0.0          0.0          0.0          0.0   \n",
            "1                     32          0.0          0.0          0.0          0.0   \n",
            "2                     32          0.0          0.0          0.0          0.0   \n",
            "3                     32          0.0          0.0          0.0          0.0   \n",
            "4                     32          0.0          0.0          0.0          0.0   \n",
            "\n",
            "   Idle Mean   Idle Std   Idle Max   Idle Min   Label  \n",
            "0        0.0        0.0        0.0        0.0  BENIGN  \n",
            "1        0.0        0.0        0.0        0.0  BENIGN  \n",
            "2        0.0        0.0        0.0        0.0  BENIGN  \n",
            "3        0.0        0.0        0.0        0.0  BENIGN  \n",
            "4        0.0        0.0        0.0        0.0  BENIGN  \n",
            "\n",
            "[5 rows x 85 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = raw.copy()\n",
        "# Rename columns: replace spaces with underscores\n",
        "df.columns = df.columns.str.replace(' ', '_')\n",
        "\n",
        "df = df[df['_Label'] != 'Heartbleed']\n",
        "\n",
        "# Now, df has its column names with spaces replaced by underscores\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDkCAUZ3F3hL",
        "outputId": "da0b6e40-1c93-4e57-a212-168e3b96cd82"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Flow_ID', '_Source_IP', '_Source_Port', '_Destination_IP',\n",
            "       '_Destination_Port', '_Protocol', '_Timestamp', '_Flow_Duration',\n",
            "       '_Total_Fwd_Packets', '_Total_Backward_Packets',\n",
            "       'Total_Length_of_Fwd_Packets', '_Total_Length_of_Bwd_Packets',\n",
            "       '_Fwd_Packet_Length_Max', '_Fwd_Packet_Length_Min',\n",
            "       '_Fwd_Packet_Length_Mean', '_Fwd_Packet_Length_Std',\n",
            "       'Bwd_Packet_Length_Max', '_Bwd_Packet_Length_Min',\n",
            "       '_Bwd_Packet_Length_Mean', '_Bwd_Packet_Length_Std', 'Flow_Bytes/s',\n",
            "       '_Flow_Packets/s', '_Flow_IAT_Mean', '_Flow_IAT_Std', '_Flow_IAT_Max',\n",
            "       '_Flow_IAT_Min', 'Fwd_IAT_Total', '_Fwd_IAT_Mean', '_Fwd_IAT_Std',\n",
            "       '_Fwd_IAT_Max', '_Fwd_IAT_Min', 'Bwd_IAT_Total', '_Bwd_IAT_Mean',\n",
            "       '_Bwd_IAT_Std', '_Bwd_IAT_Max', '_Bwd_IAT_Min', 'Fwd_PSH_Flags',\n",
            "       '_Bwd_PSH_Flags', '_Fwd_URG_Flags', '_Bwd_URG_Flags',\n",
            "       '_Fwd_Header_Length', '_Bwd_Header_Length', 'Fwd_Packets/s',\n",
            "       '_Bwd_Packets/s', '_Min_Packet_Length', '_Max_Packet_Length',\n",
            "       '_Packet_Length_Mean', '_Packet_Length_Std', '_Packet_Length_Variance',\n",
            "       'FIN_Flag_Count', '_SYN_Flag_Count', '_RST_Flag_Count',\n",
            "       '_PSH_Flag_Count', '_ACK_Flag_Count', '_URG_Flag_Count',\n",
            "       '_CWE_Flag_Count', '_ECE_Flag_Count', '_Down/Up_Ratio',\n",
            "       '_Average_Packet_Size', '_Avg_Fwd_Segment_Size',\n",
            "       '_Avg_Bwd_Segment_Size', '_Fwd_Header_Length.1', 'Fwd_Avg_Bytes/Bulk',\n",
            "       '_Fwd_Avg_Packets/Bulk', '_Fwd_Avg_Bulk_Rate', '_Bwd_Avg_Bytes/Bulk',\n",
            "       '_Bwd_Avg_Packets/Bulk', 'Bwd_Avg_Bulk_Rate', 'Subflow_Fwd_Packets',\n",
            "       '_Subflow_Fwd_Bytes', '_Subflow_Bwd_Packets', '_Subflow_Bwd_Bytes',\n",
            "       'Init_Win_bytes_forward', '_Init_Win_bytes_backward',\n",
            "       '_act_data_pkt_fwd', '_min_seg_size_forward', 'Active_Mean',\n",
            "       '_Active_Std', '_Active_Max', '_Active_Min', 'Idle_Mean', '_Idle_Std',\n",
            "       '_Idle_Max', '_Idle_Min', '_Label'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of rows for each unique label\n",
        "label_counts = df['_Label'].value_counts()\n",
        "label_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap9WoXHzGPgR",
        "outputId": "e00df928-c2da-423b-9b2d-e11827a4c1d8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN              440031\n",
              "DoS Hulk            231073\n",
              "DoS GoldenEye        10293\n",
              "DoS slowloris         5796\n",
              "DoS Slowhttptest      5499\n",
              "Name: _Label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRANSFORMER - MULTI LABEL"
      ],
      "metadata": {
        "id": "SLypyK40wmU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, precision_score, f1_score, roc_curve\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "\n",
        "# One-hot encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(df['_Label'])\n",
        "y = to_categorical(integer_encoded)\n",
        "\n",
        "# You may want to drop non-numeric columns for simplicity, but consider if any could be useful for your task\n",
        "df.drop(['Flow_ID', '_Source_IP', '_Destination_IP', '_Timestamp'], axis=1, inplace=True)\n",
        "\n",
        "# Replace inf/-inf with NaN and handle NaN values\n",
        "df = df.replace([np.inf, -np.inf], np.nan)\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "# Prepare the feature matrix X\n",
        "X = df.drop('_Label', axis=1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE()\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_smote)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Transformer block\n",
        "def transformer_block(inputs, num_heads, ff_dim, rate=0.1):\n",
        "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(inputs, inputs)\n",
        "    attn_output = Dropout(rate)(attn_output)\n",
        "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
        "    ffn_output = Dense(ff_dim, activation=\"relu\")(out1)\n",
        "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
        "    ffn_output = Dropout(rate)(ffn_output)\n",
        "    return LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
        "\n",
        "def build_model(input_shape, num_heads, ff_dim, num_classes):\n",
        "    inputs = Input(shape=(1, input_shape))  # Adjusted input shape\n",
        "    x = transformer_block(inputs, num_heads, ff_dim)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(20, activation=\"relu\")(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    outputs = Dense(num_classes, activation='sigmoid')(x)  # Output layer for multi-label\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Model configuration\n",
        "num_classes = y_train_smote.shape[1]\n",
        "model = build_model(X_train_scaled.shape[1], num_heads=3, ff_dim=24, num_classes=num_classes)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Adjust the training data shape for the Transformer model\n",
        "X_train_transformed = np.expand_dims(X_train_scaled, axis=1)\n",
        "X_test_transformed = np.expand_dims(X_test_scaled, axis=1)\n",
        "\n",
        "# Initialize EarlyStopping and ModelCheckpoint callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_transformer_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "# Train the model with more epochs and callbacks for early stopping and model checkpoint\n",
        "start_time = time.time()\n",
        "history = model.fit(\n",
        "    X_train_transformed, y_train_smote,\n",
        "    batch_size=32,\n",
        "    epochs=50,  # Increase the number of epochs\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping, model_checkpoint]  # Add callbacks\n",
        ")\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = model.evaluate(X_test_transformed, y_test)\n",
        "\n",
        "print(f\"Training Time: {training_time} seconds\")\n"
      ],
      "metadata": {
        "id": "OOcJ5gxQwsf_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "535dfcc3-1347-49f0-ede5-bdc574e6511a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-7366335ead40>:26: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  df.fillna(df.mean(), inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "43995/43995 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9904\n",
            "Epoch 1: val_loss improved from inf to 0.07922, saving model to best_transformer_model.h5\n",
            "43995/43995 [==============================] - 236s 5ms/step - loss: 0.0133 - accuracy: 0.9904 - val_loss: 0.0792 - val_accuracy: 0.9225\n",
            "Epoch 2/50\n",
            "   22/43995 [..............................] - ETA: 3:41 - loss: 0.0086 - accuracy: 0.9943"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43992/43995 [============================>.] - ETA: 0s - loss: 0.0086 - accuracy: 0.9939\n",
            "Epoch 2: val_loss did not improve from 0.07922\n",
            "43995/43995 [==============================] - 233s 5ms/step - loss: 0.0086 - accuracy: 0.9939 - val_loss: 0.1860 - val_accuracy: 0.8630\n",
            "Epoch 3/50\n",
            "43989/43995 [============================>.] - ETA: 0s - loss: 0.0069 - accuracy: 0.9952\n",
            "Epoch 3: val_loss did not improve from 0.07922\n",
            "43995/43995 [==============================] - 234s 5ms/step - loss: 0.0069 - accuracy: 0.9952 - val_loss: 0.0871 - val_accuracy: 0.9362\n",
            "Epoch 4/50\n",
            "43985/43995 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9959\n",
            "Epoch 4: val_loss improved from 0.07922 to 0.05109, saving model to best_transformer_model.h5\n",
            "43995/43995 [==============================] - 233s 5ms/step - loss: 0.0061 - accuracy: 0.9959 - val_loss: 0.0511 - val_accuracy: 0.9653\n",
            "Epoch 5/50\n",
            "43992/43995 [============================>.] - ETA: 0s - loss: 0.0055 - accuracy: 0.9964\n",
            "Epoch 5: val_loss did not improve from 0.05109\n",
            "43995/43995 [==============================] - 233s 5ms/step - loss: 0.0055 - accuracy: 0.9964 - val_loss: 0.0522 - val_accuracy: 0.9677\n",
            "Epoch 6/50\n",
            "43995/43995 [==============================] - ETA: 0s - loss: 0.0050 - accuracy: 0.9968\n",
            "Epoch 6: val_loss did not improve from 0.05109\n",
            "43995/43995 [==============================] - 233s 5ms/step - loss: 0.0050 - accuracy: 0.9968 - val_loss: 0.0728 - val_accuracy: 0.9447\n",
            "Epoch 7/50\n",
            "43989/43995 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9969\n",
            "Epoch 7: val_loss did not improve from 0.05109\n",
            "43995/43995 [==============================] - 233s 5ms/step - loss: 0.0048 - accuracy: 0.9969 - val_loss: 0.0869 - val_accuracy: 0.9485\n",
            "Epoch 8/50\n",
            "43991/43995 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9970\n",
            "Epoch 8: val_loss did not improve from 0.05109\n",
            "43995/43995 [==============================] - 233s 5ms/step - loss: 0.0046 - accuracy: 0.9970 - val_loss: 0.1115 - val_accuracy: 0.9449\n",
            "Epoch 9/50\n",
            "43987/43995 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9971Restoring model weights from the end of the best epoch: 4.\n",
            "\n",
            "Epoch 9: val_loss did not improve from 0.05109\n",
            "43995/43995 [==============================] - 233s 5ms/step - loss: 0.0044 - accuracy: 0.9971 - val_loss: 0.0524 - val_accuracy: 0.9664\n",
            "Epoch 9: early stopping\n",
            "4330/4330 [==============================] - 12s 3ms/step - loss: 0.0111 - accuracy: 0.9903\n",
            "Training Time: 2101.0584704875946 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate TPR and FPR for each label\n",
        "def calculate_tpr_fpr_per_label(y_true, y_pred):\n",
        "    tpr_list = []\n",
        "    fpr_list = []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true[:, i], y_pred[:, i]).ravel()\n",
        "        tpr = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "        fpr = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
        "        tpr_list.append(tpr)\n",
        "        fpr_list.append(fpr)\n",
        "    return tpr_list, fpr_list\n",
        "\n",
        "# Function to calculate performance metrics for each label\n",
        "def calculate_performance_metrics_per_label(y_true, y_pred):\n",
        "    tpr_list, fpr_list = calculate_tpr_fpr_per_label(y_true, y_pred)\n",
        "    precision_list = []\n",
        "    recall_list = []\n",
        "    f1_list = []\n",
        "\n",
        "    for i in range(y_true.shape[1]):\n",
        "        precision = precision_score(y_true[:, i], y_pred[:, i])\n",
        "        recall = recall_score(y_true[:, i], y_pred[:, i])\n",
        "        f1 = f1_score(y_true[:, i], y_pred[:, i])\n",
        "        precision_list.append(precision)\n",
        "        recall_list.append(recall)\n",
        "        f1_list.append(f1)\n",
        "\n",
        "    return {\n",
        "        'True Positive Rate': tpr_list,\n",
        "        'False Positive Rate': fpr_list,\n",
        "        'Precision': precision_list,\n",
        "        'Recall': recall_list,\n",
        "        'F1 Score': f1_list\n",
        "    }\n",
        "\n",
        "# Predictions for multi-label classification\n",
        "y_pred = model.predict(X_test_transformed)\n",
        "y_pred_class = (y_pred > 0.5).astype('int32')\n",
        "\n",
        "# Calculate performance metrics\n",
        "metrics_per_label = calculate_performance_metrics_per_label(y_test, y_pred_class)\n",
        "metrics_per_label['Training Time (seconds)'] = training_time\n",
        "metrics_per_label['Model Evaluation Loss'] = eval_results[0]\n",
        "metrics_per_label['Model Evaluation Accuracy'] = eval_results[1]\n",
        "\n",
        "# Print or save the results\n",
        "print(\"Transformer Performance Metrics Per Label:\", metrics_per_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSmocWubXNy9",
        "outputId": "e2a5c844-39e9-4cf7-cd7d-4a42d8c8cc71"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4330/4330 [==============================] - 10s 2ms/step\n",
            "Transformer Performance Metrics Per Label: {'True Positive Rate': [0.9857501334150855, 0.998533724340176, 0.9988316240425809, 0.9944954128440368, 0.9452423698384201], 'False Positive Rate': [0.000495363398589205, 0.000798575751137421, 0.011167556677245697, 0.0010913138691441917, 8.732035655812261e-05], 'Precision': [0.9997121175481627, 0.9493494423791822, 0.9781544655154147, 0.8784440842787682, 0.9887323943661972], 'Recall': [0.9857501334150855, 0.998533724340176, 0.9988316240425809, 0.9944954128440368, 0.9452423698384201], 'F1 Score': [0.9926820343944384, 0.9733206288708909, 0.9883849143052894, 0.9328743545611016, 0.9664983937586048], 'Training Time (seconds)': 2101.0584704875946, 'Model Evaluation Loss': 0.011089515872299671, 'Model Evaluation Accuracy': 0.9903131723403931}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the LabelEncoder to the labels\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(df['_Label'])\n",
        "\n",
        "# Get the list of original class names\n",
        "label_names = label_encoder.classes_\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "def calculate_metrics_per_label(y_true, y_pred, label_names):\n",
        "    metrics = {}\n",
        "    for i, label in enumerate(label_names):\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true[:, i], y_pred[:, i]).ravel()\n",
        "\n",
        "        # Calculating TPR and FPR\n",
        "        tpr = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "        fpr = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
        "\n",
        "        # Calculating Precision and F1 Score\n",
        "        precision = precision_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
        "        f1 = f1_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
        "\n",
        "        metrics[label] = {'TPR': tpr, 'FPR': fpr, 'Precision': precision, 'F1 Score': f1}\n",
        "    return metrics\n",
        "\n",
        "def create_confusion_matrix_grid(y_true, y_pred, label_names):\n",
        "    # Convert label_names to a list if it's not already\n",
        "    if isinstance(label_names, np.ndarray):\n",
        "        label_names = label_names.tolist()\n",
        "\n",
        "    # Initialize an empty DataFrame to store the confusion matrix grid\n",
        "    confusion_matrix_grid = pd.DataFrame(index=label_names, columns=label_names)\n",
        "\n",
        "    for actual_label in label_names:\n",
        "        actual_index = label_names.index(actual_label)\n",
        "        for predicted_label in label_names:\n",
        "            predicted_index = label_names.index(predicted_label)\n",
        "\n",
        "            # Calculate the count for actual vs predicted label\n",
        "            count = sum((y_true[:, actual_index] == 1) & (y_pred[:, predicted_index] == 1))\n",
        "            confusion_matrix_grid.at[actual_label, predicted_label] = count\n",
        "\n",
        "    return confusion_matrix_grid\n",
        "\n",
        "\n",
        "# Example usage\n",
        "metrics_per_label = calculate_metrics_per_label(y_test, y_pred_class, label_names)\n",
        "\n"
      ],
      "metadata": {
        "id": "0diHS8sHL6Qn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the corresponding label names\n",
        "print(\"Label Encoding Order:\", label_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCYUjpBXN2a1",
        "outputId": "290e5b96-f26f-4e84-c0a1-7f625ac0aa4d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Encoding Order: ['BENIGN' 'DoS GoldenEye' 'DoS Hulk' 'DoS Slowhttptest' 'DoS slowloris']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_per_label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShQVprSML-GP",
        "outputId": "d20a045f-33d3-465f-e170-7bdad52e0eb3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BENIGN': {'TPR': 0.9857501334150855,\n",
              "  'FPR': 0.000495363398589205,\n",
              "  'Precision': 0.9997121175481627,\n",
              "  'F1 Score': 0.9926820343944384},\n",
              " 'DoS GoldenEye': {'TPR': 0.998533724340176,\n",
              "  'FPR': 0.000798575751137421,\n",
              "  'Precision': 0.9493494423791822,\n",
              "  'F1 Score': 0.9733206288708909},\n",
              " 'DoS Hulk': {'TPR': 0.9988316240425809,\n",
              "  'FPR': 0.011167556677245697,\n",
              "  'Precision': 0.9781544655154147,\n",
              "  'F1 Score': 0.9883849143052894},\n",
              " 'DoS Slowhttptest': {'TPR': 0.9944954128440368,\n",
              "  'FPR': 0.0010913138691441917,\n",
              "  'Precision': 0.8784440842787682,\n",
              "  'F1 Score': 0.9328743545611016},\n",
              " 'DoS slowloris': {'TPR': 0.9452423698384201,\n",
              "  'FPR': 8.732035655812261e-05,\n",
              "  'Precision': 0.9887323943661972,\n",
              "  'F1 Score': 0.9664983937586048}}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix_grid = create_confusion_matrix_grid(y_test, y_pred_class, label_names)\n",
        "\n",
        "print(confusion_matrix_grid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeHVrvgccjYX",
        "outputId": "e80cda8e-e77b-4389-cc58-2863014dd3ac"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 BENIGN DoS GoldenEye DoS Hulk DoS Slowhttptest DoS slowloris\n",
            "BENIGN            86816            74     1029              106            11\n",
            "DoS GoldenEye         0          2043        1                2             0\n",
            "DoS Hulk             20            31    46164                0             0\n",
            "DoS Slowhttptest      3             3        0             1084             1\n",
            "DoS slowloris         2             1        1               42          1053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to calculate TPR and FPR\n",
        "def calculate_tpr_fpr(y_true, y_pred):\n",
        "    # Calculate confusion matrix for each label\n",
        "    tpr_list = []\n",
        "    fpr_list = []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true[:, i], y_pred[:, i]).ravel()\n",
        "        tpr = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "        fpr = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
        "        tpr_list.append(tpr)\n",
        "        fpr_list.append(fpr)\n",
        "\n",
        "    # Return average TPR and FPR\n",
        "    return np.mean(tpr_list), np.mean(fpr_list)\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "# Function to calculate performance metrics\n",
        "def calculate_performance_metrics(y_true, y_pred):\n",
        "    # Calculate TPR and FPR\n",
        "    tpr, fpr = calculate_tpr_fpr(y_true, y_pred)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score for each label and find their average\n",
        "    precision = precision_score(y_true, y_pred, average='micro')\n",
        "    recall = recall_score(y_true, y_pred, average='micro')\n",
        "    f1 = f1_score(y_true, y_pred, average='micro')\n",
        "\n",
        "    return {\n",
        "        'True Positive Rate': tpr,\n",
        "        'False Positive Rate': fpr,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1 Score': f1\n",
        "    }\n",
        "\n",
        "# Predictions for multi-label classification\n",
        "y_pred = model.predict(X_test_transformed)\n",
        "y_pred_class = (y_pred > 0.5).astype('int32')\n",
        "\n",
        "# Calculate performance metrics\n",
        "metrics = calculate_performance_metrics(y_test, y_pred_class)\n",
        "metrics['Training Time (seconds)'] = training_time\n",
        "metrics['Model Evaluation Loss'] = eval_results[0]\n",
        "metrics['Model Evaluation Accuracy'] = eval_results[1]\n",
        "\n",
        "# Print or save the results\n",
        "print(\"Transformer Performance Metrics:\", metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwGwZ_mTs5L9",
        "outputId": "b0f525e2-b48f-4277-8cbf-ea60853e266e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4330/4330 [==============================] - 10s 2ms/step\n",
            "Transformer Performance Metrics: {'True Positive Rate': 0.9988906528960598, 'False Positive Rate': 0.00103280260105349277, 'Precision': 0.9985578731577693, 'Recall': 0.9964461241960747, 'F1 Score': 0.9987019637867926, 'Training Time (seconds)': 2101.0584704875946, 'Model Evaluation Loss': 0.0033789515872299671, 'Model Evaluation Accuracy': 0.996681723403931}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate TPR and FPR for each label\n",
        "def calculate_tpr_fpr_per_label(y_true, y_pred):\n",
        "    tpr_list = []\n",
        "    fpr_list = []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true[:, i], y_pred[:, i]).ravel()\n",
        "        tpr = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "        fpr = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
        "        tpr_list.append(tpr)\n",
        "        fpr_list.append(fpr)\n",
        "    return tpr_list, fpr_list\n",
        "\n",
        "# Function to calculate performance metrics for each label\n",
        "def calculate_performance_metrics_per_label(y_true, y_pred):\n",
        "    tpr_list, fpr_list = calculate_tpr_fpr_per_label(y_true, y_pred)\n",
        "    precision_list = []\n",
        "    recall_list = []\n",
        "    f1_list = []\n",
        "\n",
        "    for i in range(y_true.shape[1]):\n",
        "        precision = precision_score(y_true[:, i], y_pred[:, i])\n",
        "        recall = recall_score(y_true[:, i], y_pred[:, i])\n",
        "        f1 = f1_score(y_true[:, i], y_pred[:, i])\n",
        "        precision_list.append(precision)\n",
        "        recall_list.append(recall)\n",
        "        f1_list.append(f1)\n",
        "\n",
        "    return {\n",
        "        'True Positive Rate': tpr_list,\n",
        "        'False Positive Rate': fpr_list,\n",
        "        'Precision': precision_list,\n",
        "        'Recall': recall_list,\n",
        "        'F1 Score': f1_list\n",
        "    }\n",
        "\n",
        "# Predictions for multi-label classification\n",
        "y_pred = model.predict(X_test_transformed)\n",
        "y_pred_class = (y_pred > 0.5).astype('int32')\n",
        "\n",
        "# Calculate performance metrics\n",
        "metrics_per_label = calculate_performance_metrics_per_label(y_test, y_pred_class)\n",
        "metrics_per_label['Training Time (seconds)'] = training_time\n",
        "metrics_per_label['Model Evaluation Loss'] = eval_results[0]\n",
        "metrics_per_label['Model Evaluation Accuracy'] = eval_results[1]\n",
        "\n",
        "# Print or save the results\n",
        "print(\"Transformer Performance Metrics Per Label:\", metrics_per_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg6etKXUX60F",
        "outputId": "cc11ba39-28fe-4891-93dd-840264bc6b78"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4330/4330 [==============================] - 10s 2ms/step\n",
            "Transformer Performance Metrics Per Label: {'True Positive Rate': [0.9857501334150855, 0.998533724340176, 0.9988316240425809, 0.9944954128440368, 0.9452423698384201], 'False Positive Rate': [0.000495363398589205, 0.000798575751137421, 0.011167556677245697, 0.0010913138691441917, 8.732035655812261e-05], 'Precision': [0.9997121175481627, 0.9493494423791822, 0.9781544655154147, 0.8784440842787682, 0.9887323943661972], 'Recall': [0.9857501334150855, 0.998533724340176, 0.9988316240425809, 0.9944954128440368, 0.9452423698384201], 'F1 Score': [0.9926820343944384, 0.9733206288708909, 0.9883849143052894, 0.9328743545611016, 0.9664983937586048], 'Training Time (seconds)': 2101.0584704875946, 'Model Evaluation Loss': 0.011089515872299671, 'Model Evaluation Accuracy': 0.9903131723403931}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEEP NEURAL NETWORK - MULTI LABEL"
      ],
      "metadata": {
        "id": "pPeXgGtrwxpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, hamming_loss\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "# Assume df is your DataFrame\n",
        "# Convert categorical labels to a format suitable for multi-label classification\n",
        "mlb = MultiLabelBinarizer()\n",
        "y = mlb.fit_transform(df[['_Label']].values)\n",
        "\n",
        "# Identify feature columns\n",
        "feature_columns = df.columns.drop(['_Label'])\n",
        "\n",
        "# Drop non-relevant columns and preprocess features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(df[feature_columns])\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the DNN Model\n",
        "dnn_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(y_train.shape[1], activation='sigmoid')  # Output layer for multi-label\n",
        "])\n",
        "\n",
        "dnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Measure training time\n",
        "start_time = time.time()\n",
        "dnn_history = dnn_model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.2)\n",
        "dnn_training_time = time.time() - start_time\n",
        "\n",
        "# Evaluate the model\n",
        "dnn_eval_results = dnn_model.evaluate(X_test, y_test)\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate TPR and FPR for multi-label classification\n",
        "def calculate_tpr_fpr(y_true, y_pred):\n",
        "    tpr_list = []\n",
        "    fpr_list = []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true[:, i], y_pred[:, i]).ravel()\n",
        "        tpr = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "        fpr = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
        "        tpr_list.append(tpr)\n",
        "        fpr_list.append(fpr)\n",
        "    return np.mean(tpr_list), np.mean(fpr_list)\n",
        "\n",
        "# Function to calculate performance metrics for multi-label classification\n",
        "def calculate_performance_metrics(y_true, y_pred, training_time, eval_results):\n",
        "    tpr, fpr = calculate_tpr_fpr(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='micro')\n",
        "    recall = recall_score(y_true, y_pred, average='micro')\n",
        "    f1 = f1_score(y_true, y_pred, average='micro')\n",
        "\n",
        "    return {\n",
        "        'True Positive Rate': tpr,\n",
        "        'False Positive Rate': fpr,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1 Score': f1,\n",
        "        'Training Time (seconds)': training_time,\n",
        "        'Model Evaluation Loss': eval_results[0],\n",
        "        'Model Evaluation Accuracy': eval_results[1]\n",
        "    }\n",
        "\n",
        "# Predict and calculate multi-label metrics for DNN\n",
        "dnn_y_pred = (dnn_model.predict(X_test) > 0.5).astype('int32')\n",
        "dnn_performance = calculate_performance_metrics(y_test, dnn_y_pred, dnn_training_time, dnn_eval_results)\n",
        "\n",
        "print(\"DNN Performance Metrics:\", dnn_performance)"
      ],
      "metadata": {
        "id": "fMfTnUeNw2E0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc74756d-9ff6-442c-ea49-84a770eaa655"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "13854/13854 [==============================] - 46s 3ms/step - loss: 0.0190 - accuracy: 0.9842 - val_loss: 0.0119 - val_accuracy: 0.9899\n",
            "Epoch 2/10\n",
            "13854/13854 [==============================] - 45s 3ms/step - loss: 0.0123 - accuracy: 0.9896 - val_loss: 0.0098 - val_accuracy: 0.9911\n",
            "Epoch 3/10\n",
            "13854/13854 [==============================] - 44s 3ms/step - loss: 0.0114 - accuracy: 0.9901 - val_loss: 0.0108 - val_accuracy: 0.9913\n",
            "Epoch 4/10\n",
            "13854/13854 [==============================] - 45s 3ms/step - loss: 0.0109 - accuracy: 0.9905 - val_loss: 0.0090 - val_accuracy: 0.9915\n",
            "Epoch 5/10\n",
            "13854/13854 [==============================] - 45s 3ms/step - loss: 0.0103 - accuracy: 0.9907 - val_loss: 0.0087 - val_accuracy: 0.9917\n",
            "Epoch 6/10\n",
            "13854/13854 [==============================] - 45s 3ms/step - loss: 0.0102 - accuracy: 0.9910 - val_loss: 0.0083 - val_accuracy: 0.9918\n",
            "Epoch 7/10\n",
            "13854/13854 [==============================] - 45s 3ms/step - loss: 0.0100 - accuracy: 0.9912 - val_loss: 0.0085 - val_accuracy: 0.9923\n",
            "Epoch 8/10\n",
            "13854/13854 [==============================] - 45s 3ms/step - loss: 0.0096 - accuracy: 0.9915 - val_loss: 0.0077 - val_accuracy: 0.9920\n",
            "Epoch 9/10\n",
            "13854/13854 [==============================] - 44s 3ms/step - loss: 0.0093 - accuracy: 0.9918 - val_loss: 0.0081 - val_accuracy: 0.9920\n",
            "Epoch 10/10\n",
            "13854/13854 [==============================] - 45s 3ms/step - loss: 0.0091 - accuracy: 0.9921 - val_loss: 0.0076 - val_accuracy: 0.9925\n",
            "4330/4330 [==============================] - 8s 2ms/step - loss: 0.0075 - accuracy: 0.9924\n",
            "4330/4330 [==============================] - 6s 1ms/step\n",
            "DNN Performance Metrics: {'True Positive Rate': 0.9893638076365512, 'False Positive Rate': 0.0022206012696232543, 'Precision': 0.9926780130987024, 'Recall': 0.9923054158034922, 'F1 Score': 0.9924916794813483, 'Training Time (seconds)': 449.50518918037415, 'Model Evaluation Loss': 0.0074510956183075905, 'Model Evaluation Accuracy': 0.9923847913742065}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONVOLUTIONAL NEURAL NETWORK - MULTI LABEL"
      ],
      "metadata": {
        "id": "INzlIMOPw5_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape input for CNN\n",
        "X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Build the CNN Model\n",
        "cnn_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(50, activation='relu'),\n",
        "    tf.keras.layers.Dense(y_train.shape[1], activation='sigmoid')  # Output layer for multi-label\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the CNN Model\n",
        "start_time = time.time()\n",
        "cnn_history = cnn_model.fit(X_train_cnn, y_train, batch_size=32, epochs=10, validation_split=0.2)\n",
        "cnn_training_time = time.time() - start_time\n",
        "\n",
        "# Evaluate the CNN Model\n",
        "cnn_eval_results = cnn_model.evaluate(X_test_cnn, y_test)\n",
        "\n",
        "# Predict and calculate multi-label metrics for CNN\n",
        "cnn_y_pred = (cnn_model.predict(X_test_cnn) > 0.5).astype('int32')"
      ],
      "metadata": {
        "id": "FzyzjEOcw5uL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "725a232f-61cf-44c7-9479-bc656f82ae33"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "13854/13854 [==============================] - 47s 3ms/step - loss: 0.0178 - accuracy: 0.9855 - val_loss: 0.0123 - val_accuracy: 0.9909\n",
            "Epoch 2/10\n",
            "13854/13854 [==============================] - 47s 3ms/step - loss: 0.0111 - accuracy: 0.9906 - val_loss: 0.0096 - val_accuracy: 0.9913\n",
            "Epoch 3/10\n",
            "13854/13854 [==============================] - 46s 3ms/step - loss: 0.0100 - accuracy: 0.9914 - val_loss: 0.0086 - val_accuracy: 0.9916\n",
            "Epoch 4/10\n",
            "13854/13854 [==============================] - 46s 3ms/step - loss: 0.0089 - accuracy: 0.9924 - val_loss: 0.0081 - val_accuracy: 0.9945\n",
            "Epoch 5/10\n",
            "13854/13854 [==============================] - 46s 3ms/step - loss: 0.0078 - accuracy: 0.9938 - val_loss: 0.0079 - val_accuracy: 0.9945\n",
            "Epoch 6/10\n",
            "13854/13854 [==============================] - 46s 3ms/step - loss: 0.0068 - accuracy: 0.9950 - val_loss: 0.0075 - val_accuracy: 0.9946\n",
            "Epoch 7/10\n",
            "13854/13854 [==============================] - 46s 3ms/step - loss: 0.0063 - accuracy: 0.9957 - val_loss: 0.0053 - val_accuracy: 0.9965\n",
            "Epoch 8/10\n",
            "13854/13854 [==============================] - 46s 3ms/step - loss: 0.0059 - accuracy: 0.9960 - val_loss: 0.0058 - val_accuracy: 0.9963\n",
            "Epoch 9/10\n",
            "13854/13854 [==============================] - 46s 3ms/step - loss: 0.0056 - accuracy: 0.9963 - val_loss: 0.0049 - val_accuracy: 0.9964\n",
            "Epoch 10/10\n",
            "13854/13854 [==============================] - 46s 3ms/step - loss: 0.0053 - accuracy: 0.9965 - val_loss: 0.0050 - val_accuracy: 0.9968\n",
            "4330/4330 [==============================] - 8s 2ms/step - loss: 0.0054 - accuracy: 0.9966\n",
            "4330/4330 [==============================] - 6s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, hamming_loss, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "def calculate_tpr_fpr(y_true, y_pred):\n",
        "    tpr_list = []\n",
        "    fpr_list = []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true[:, i], y_pred[:, i]).ravel()\n",
        "        tpr = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "        fpr = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
        "        tpr_list.append(tpr)\n",
        "        fpr_list.append(fpr)\n",
        "    return np.mean(tpr_list), np.mean(fpr_list)\n",
        "\n",
        "def calculate_performance_metrics(y_true, y_pred, training_time, eval_results):\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # Calculate Hamming loss\n",
        "    ham_loss = hamming_loss(y_true, y_pred)\n",
        "\n",
        "    # Calculate micro-averaged Precision, Recall, and F1 Score for multi-label classification\n",
        "    precision = precision_score(y_true, y_pred, average='micro')\n",
        "    recall = recall_score(y_true, y_pred, average='micro')\n",
        "    f1 = f1_score(y_true, y_pred, average='micro')\n",
        "\n",
        "    # Calculate average TPR and FPR for multi-label classification\n",
        "    tpr, fpr = calculate_tpr_fpr(y_true, y_pred)\n",
        "\n",
        "    # Collecting all metrics\n",
        "    performance_metrics = {\n",
        "        'Accuracy': accuracy,\n",
        "        'Hamming Loss': ham_loss,\n",
        "        'True Positive Rate': tpr,\n",
        "        'False Positive Rate': fpr,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1 Score': f1,\n",
        "        'Training Time (seconds)': training_time,\n",
        "        'Model Evaluation Loss': eval_results[0],\n",
        "        'Model Evaluation Accuracy': eval_results[1]\n",
        "    }\n",
        "\n",
        "    return performance_metrics\n",
        "\n",
        "# Assuming cnn_y_pred, cnn_training_time, cnn_eval_results are already defined\n",
        "cnn_performance = calculate_performance_metrics(y_test, cnn_y_pred, cnn_training_time, cnn_eval_results)\n",
        "print(\"CNN Performance Metrics:\", cnn_performance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSd0AzguaNSB",
        "outputId": "fcfc7b99-79c4-4d99-8857-3f7d0967d1e0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN Performance Metrics: {'Accuracy': 0.9964847443680119, 'Hamming Loss': 0.0013469131435913354, 'True Positive Rate': 0.9913748655728847, 'False Positive Rate': 0.0010880991146476483, 'Precision': 0.996578951167054, 'Recall': 0.9966868535213911, 'F1 Score': 0.9966328994236592, 'Training Time (seconds)': 462.691410779953, 'Model Evaluation Loss': 0.0054249232634902, 'Model Evaluation Accuracy': 0.9966291189193726}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LONG SHORT TERM MEMORY - MULTI LABEL"
      ],
      "metadata": {
        "id": "S-MzZjPaUNq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape input for LSTM\n",
        "X_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_lstm = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# Build the LSTM Model\n",
        "lstm_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(50, return_sequences=True, input_shape=(1, X_train.shape[1])),\n",
        "    tf.keras.layers.LSTM(50),\n",
        "    tf.keras.layers.Dense(y_train.shape[1], activation='sigmoid')  # Output layer for multi-label\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the LSTM Model\n",
        "start_time = time.time()\n",
        "lstm_history = lstm_model.fit(X_train_lstm, y_train, batch_size=32, epochs=10, validation_split=0.2)\n",
        "lstm_training_time = time.time() - start_time\n",
        "\n",
        "# Evaluate the LSTM Model\n",
        "lstm_eval_results = lstm_model.evaluate(X_test_lstm, y_test)\n",
        "\n",
        "# Predict and calculate multi-label metrics for LSTM\n",
        "lstm_y_pred = (lstm_model.predict(X_test_lstm) > 0.5).astype('int32')\n",
        "lstm_performance = calculate_performance_metrics(y_test, lstm_y_pred, lstm_training_time, lstm_eval_results)\n",
        "\n",
        "print(\"LSTM Performance Metrics:\", lstm_performance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rATl1jvWTtnP",
        "outputId": "96f7aa2b-cf89-4d74-a0e6-d28764c44c16"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "13854/13854 [==============================] - 73s 5ms/step - loss: 0.0183 - accuracy: 0.9857 - val_loss: 0.0099 - val_accuracy: 0.9909\n",
            "Epoch 2/10\n",
            "13854/13854 [==============================] - 69s 5ms/step - loss: 0.0100 - accuracy: 0.9909 - val_loss: 0.0090 - val_accuracy: 0.9917\n",
            "Epoch 3/10\n",
            "13854/13854 [==============================] - 69s 5ms/step - loss: 0.0093 - accuracy: 0.9913 - val_loss: 0.0112 - val_accuracy: 0.9797\n",
            "Epoch 4/10\n",
            "13854/13854 [==============================] - 69s 5ms/step - loss: 0.0087 - accuracy: 0.9918 - val_loss: 0.0085 - val_accuracy: 0.9919\n",
            "Epoch 5/10\n",
            "13854/13854 [==============================] - 69s 5ms/step - loss: 0.0082 - accuracy: 0.9922 - val_loss: 0.0077 - val_accuracy: 0.9919\n",
            "Epoch 6/10\n",
            "13854/13854 [==============================] - 69s 5ms/step - loss: 0.0077 - accuracy: 0.9928 - val_loss: 0.0087 - val_accuracy: 0.9922\n",
            "Epoch 7/10\n",
            "13854/13854 [==============================] - 69s 5ms/step - loss: 0.0073 - accuracy: 0.9933 - val_loss: 0.0135 - val_accuracy: 0.9795\n",
            "Epoch 8/10\n",
            "13854/13854 [==============================] - 70s 5ms/step - loss: 0.0069 - accuracy: 0.9938 - val_loss: 0.0061 - val_accuracy: 0.9960\n",
            "Epoch 9/10\n",
            "13854/13854 [==============================] - 69s 5ms/step - loss: 0.0066 - accuracy: 0.9941 - val_loss: 0.0055 - val_accuracy: 0.9958\n",
            "Epoch 10/10\n",
            "13854/13854 [==============================] - 69s 5ms/step - loss: 0.0063 - accuracy: 0.9945 - val_loss: 0.0059 - val_accuracy: 0.9952\n",
            "4330/4330 [==============================] - 11s 2ms/step - loss: 0.0058 - accuracy: 0.9955\n",
            "4330/4330 [==============================] - 9s 2ms/step\n",
            "LSTM Performance Metrics: {'Accuracy': 0.9951926894231949, 'Hamming Loss': 0.001850742390229466, 'True Positive Rate': 0.9870233450182557, 'False Positive Rate': 0.001613543299618896, 'Precision': 0.9954089035508811, 'Recall': 0.9953370531041801, 'F1 Score': 0.9953729770309094, 'Training Time (seconds)': 693.9536406993866, 'Model Evaluation Loss': 0.005764384288340807, 'Model Evaluation Accuracy': 0.9954525232315063}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, hamming_loss, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "def calculate_tpr_fpr(y_true, y_pred):\n",
        "    # Calculate TPR and FPR for each label and average them\n",
        "    tpr_list, fpr_list = [], []\n",
        "    for i in range(y_true.shape[1]):\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true[:, i], y_pred[:, i]).ravel()\n",
        "        tpr = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "        fpr = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
        "        tpr_list.append(tpr)\n",
        "        fpr_list.append(fpr)\n",
        "    return np.mean(tpr_list), np.mean(fpr_list)\n",
        "\n",
        "def calculate_performance_metrics(y_true, y_pred, training_time, eval_results):\n",
        "    # Standard metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    ham_loss = hamming_loss(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='micro')\n",
        "    recall = recall_score(y_true, y_pred, average='micro')\n",
        "    f1 = f1_score(y_true, y_pred, average='micro')\n",
        "\n",
        "    # Additional metrics\n",
        "    tpr, fpr = calculate_tpr_fpr(y_true, y_pred)\n",
        "\n",
        "    # Metrics collection\n",
        "    performance_metrics = {\n",
        "        'Accuracy': accuracy,\n",
        "        'Hamming Loss': ham_loss,\n",
        "        'True Positive Rate': tpr,\n",
        "        'False Positive Rate': fpr,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1 Score': f1,\n",
        "        'Training Time (seconds)': training_time,\n",
        "        'Model Evaluation Loss': eval_results[0],\n",
        "        'Model Evaluation Accuracy': eval_results[1]\n",
        "    }\n",
        "\n",
        "    return performance_metrics\n",
        "\n",
        "# Predict and calculate multi-label metrics for LSTM\n",
        "lstm_y_pred = (lstm_model.predict(X_test_lstm) > 0.5).astype('int32')\n",
        "lstm_performance = calculate_performance_metrics(y_test, lstm_y_pred, lstm_training_time, lstm_eval_results)\n",
        "\n",
        "print(\"LSTM Performance Metrics:\", lstm_performance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCSAg0LcmAyp",
        "outputId": "95f78509-f789-433c-82a5-ab56015aaef7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4330/4330 [==============================] - 8s 2ms/step\n",
            "LSTM Performance Metrics: {'Accuracy': 0.9951926894231949, 'Hamming Loss': 0.001850742390229466, 'True Positive Rate': 0.9870233450182557, 'False Positive Rate': 0.001613543299618896, 'Precision': 0.9954089035508811, 'Recall': 0.9953370531041801, 'F1 Score': 0.9953729770309094, 'Training Time (seconds)': 693.9536406993866, 'Model Evaluation Loss': 0.005764384288340807, 'Model Evaluation Accuracy': 0.9954525232315063}\n"
          ]
        }
      ]
    }
  ]
}